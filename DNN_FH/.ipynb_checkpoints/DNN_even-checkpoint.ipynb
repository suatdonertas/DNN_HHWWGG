{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0123230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-04 21:45:01.025721: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc10-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc10-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc10-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc10-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc10-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc10-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/10.3.0-f5826/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/10.3.0-f5826/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.36.1-a9696/x86_64-centos7/lib:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/lib64:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/lib:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib64:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc10-opt/lib64/R/library/readr/rcon\n",
      "2022-02-04 21:45:01.025765: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters of the training ###\n",
    "\n",
    "split = \"even\" \n",
    "#split = \"odd\" \n",
    "# split = even | odd -> on what split to train the model (will be in the name)\n",
    "# -> you need one \"odd\" and one \"even\" models to be put inside bamboo\n",
    "\n",
    "suffix = 'test'\n",
    "# Suffix that will be added to the saved model (so multiple DNNs can be trained)\n",
    "\n",
    "quantile = 0.95 # We will repeat the part of the weights rightmost tail\n",
    "# Eg : 0.95, means we take the 5% events on the right tail of training weight and repeat them\n",
    "# 1.0 means no correction (to be used if you want to diable it)\n",
    "\n",
    "# DNN hyperparameters #\n",
    "parameters = {\n",
    "    'epochs'                : 200,\n",
    "    'lr'                    : 0.001,\n",
    "    'batch_size'            : 256,\n",
    "    'n_layers'              : 3,\n",
    "    'n_neurons'             : 64,\n",
    "    'hidden_activation'     : 'relu',\n",
    "    'output_activation'     : 'sigmoid',\n",
    "    'l2'                    : 1e-6,\n",
    "    'dropout'               : 0.,\n",
    "    'batch_norm'            : True,\n",
    "}\n",
    "# L2 is an additional term in the loss function : l2 x ||W||**2 where ||W|| is the sum of all the DNN weights \n",
    "#    inside the neurons\n",
    "# -> when overfitting the weights take large values, this tells the optimizer the trade off between performances\n",
    "#    and generalization (from experience, a small value always helps)\n",
    "# Dropout is a frequency of killing neurons at each batch (no backprogation for them)\n",
    "# -> used generally when overfitting is detected, to avoid that the DNN learns too much \n",
    "#    (from experience, not always useful, put it when you see overfitting)\n",
    "# Batch normalization is a layer that normalizes the output of each neuron (see documentation)\n",
    "# -> usually helps that the gradient does not go too far and backprogation is always smooth (supposedly)\n",
    "#    (from experience : always used it, maybe not worth all the time)\n",
    "\n",
    "\n",
    "# Input variables\n",
    "input_vars=[\"Eta_ph1\",\n",
    "            \"Phi_ph1\",\n",
    "            \"E_mGG_ph1\",\n",
    "            \"pT_mGG_ph1\",\n",
    "            \"Eta_ph2\",\n",
    "            \"Phi_ph2\",\n",
    "            \"E_mGG_ph2\",\n",
    "            \"pT_mGG_ph2\",\n",
    "            \"deltaPhi_DiPh\",\n",
    "            \"deltaR_DiPh\",\n",
    "            \"nJets\",\n",
    "            \"E_jet1\",   \n",
    "            \"pT_jet1\",\n",
    "            \"Eta_jet1\",\n",
    "            \"Phi_jet1\", \n",
    "            \"E_jet2\",   \n",
    "            \"pT_jet2\",\n",
    "            \"Eta_jet2\",\n",
    "            \"Phi_jet2\",  \n",
    "            \"E_jet3\",   \n",
    "            \"pT_jet3\",\n",
    "            \"Eta_jet3\",\n",
    "            \"Phi_jet3\",\n",
    "            \"E_jet4\",   \n",
    "            \"pT_jet4\",\n",
    "            \"Eta_jet4\",\n",
    "            \"Phi_jet4\",\n",
    "            \"w1_pT\",\n",
    "            \"w1_eta\",\n",
    "            \"w1_mass\",\n",
    "            \"w2_pT\",\n",
    "            \"w2_eta\",\n",
    "            \"w2_mass\",\n",
    "            \"ww_pT\",\n",
    "            \"ww_eta\",\n",
    "            \"ww_mass\",\n",
    "            \"mindeltaRPJ\",\n",
    "            \"maxdeltaRPJ\",\n",
    "            \"deltaRJJ\",\n",
    "            \"deltaRJJ2\",\n",
    "            \"deltaPhi_HH\",\n",
    "            \"deltaR_HH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16348b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required data #\n",
    "outputPath = '/home/ucl/cp3/sdonerta/bamboodev/WWGG/DNNOpt_FH_Skim_04_02'\n",
    "skimFile = os.path.join(outputPath,'results','Skim_FH.parquet')\n",
    "yamlFile = os.path.join(outputPath,'plots.yml')\n",
    "\n",
    "# Load dataframe from parquet #\n",
    "df = pd.read_parquet(skimFile)\n",
    "\n",
    "# Load samples + plots data from yaml file #\n",
    "with open(yamlFile,'r') as handle:\n",
    "    config = yaml.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut negative event weights #\n",
    "df = df[(df['weight']>0) & (df['weight']<300)]\n",
    "#df = df[df['weight']>0]\n",
    "\n",
    "# Cut events/samples #\n",
    "df = df[~df.process.str.contains(\"VBF\")]\n",
    "df = df[~df.process.str.contains(\"GluGluHToGG\")]\n",
    "df = df[~df.process.str.contains(\"ttH\")]\n",
    "df = df[~df.process.str.contains(\"VH\")]\n",
    "df = df[~df.process.str.contains(\"THQ\")]\n",
    "df = df[~df.process.str.contains(\"GluGluHToGG\")]\n",
    "df = df[~df.process.str.contains(\"W1\")]\n",
    "df = df[~df.process.str.contains(\"W2\")]\n",
    "df = df[~df.process.str.contains(\"W3\")]\n",
    "df = df[~df.process.str.contains(\"WGJJ\")]\n",
    "df = df[~df.process.str.contains(\"WGGJets\")]\n",
    "df = df[~df.process.str.contains(\"DY\")]\n",
    "df = df[~df.process.str.contains(\"ZG\")]\n",
    "df = df[~df.process.str.contains(\"WW\")]\n",
    "df = df[~df.process.str.contains(\"TT\")]\n",
    "df = df[~df.process.str.contains(\"TTW\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set labels #\n",
    "df[\"label\"] = 0\n",
    "for process in pd.unique(df['process']):\n",
    "    if process not in config['files']:\n",
    "        raise RuntimeError(f'Process {process} not found in yaml config file')\n",
    "    if config['files'][process]['type'] == 'signal':\n",
    "        df.loc[df['process']==process, ['label']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b641a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b51884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce physical event weight #\n",
    "# df['event_weight'] = pd.Series(np.zeros(df.shape[0]))\n",
    "# lumi = config['configuration']['luminosity']['HL-LHC']\n",
    "# for process in pd.unique(df['process']):\n",
    "#     print (f'Looking at process {process}')\n",
    "#     if 'cross-section' in config['files'][process].keys() and config['files'][process]['type'] != 'signal':\n",
    "#         cross_section = config[\"files\"][process][\"cross-section\"]\n",
    "#     else:\n",
    "#         cross_section = 1.\n",
    "#         # For signal, we assume unit cross section (otherwise significance can be too unstable)\n",
    "#     if 'branching-ratio' in config['files'][process].keys():\n",
    "#         BR = config[\"files\"][process][\"branching-ratio\"]\n",
    "#     else:\n",
    "#         BR = 1\n",
    "#     if 'generated-events' in config['files'][process].keys():\n",
    "        \n",
    "#         generated_events = config[\"files\"][process][\"generated-events\"]\n",
    "#     else:\n",
    "#         raise RuntimeError('Process {process} is missing `generated-events` entry, should not happen')\n",
    "        \n",
    "#     print (f'... cross-section = {cross_section}')\n",
    "#     print (f'... branching-ratio = {BR}')\n",
    "#     print (f'... generated-events = {generated_events}')\n",
    "#     factor = lumi * cross_section * BR / generated_events\n",
    "#     # We don't really care about luminosity because the scale of the weights do not matter, and you have a single lumi\n",
    "#     print (f'   -> Total factor = {factor}')\n",
    "#     # Apply to the new event_weight columns #\n",
    "#     df.loc[df[\"process\"]==process,'event_weight'] = df[df[\"process\"]==process]['weight'] * factor\n",
    "#     print (f'   Sum of weights = {df[df[\"process\"]==process][\"weight\"].sum()} -> {df[df[\"process\"]==process][\"event_weight\"].sum()}')\n",
    "\n",
    "df['event_weight'] = df['weight'].copy()\n",
    "\n",
    "if (df['event_weight'] < 0).sum() > 0:\n",
    "    raise RuntimeError(f\"There are {(df['event_weight'] < 0).sum()} events with negative event weight, this should not happen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c50d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training weight #\n",
    "if 'training_weight' in df.columns:\n",
    "    del df['training_weight']\n",
    "df['training_weight'] = df['event_weight'].copy()\n",
    "\n",
    "df.loc[df['label']==1,'training_weight'] *= df.shape[0]/2 / (df[df['label']==1]['event_weight']).sum()\n",
    "df.loc[df['label']==0,'training_weight'] *= df.shape[0]/2 / (df[df['label']==0]['event_weight']).sum()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils) # Reload in case file has changed\n",
    "print ('Using event weight')\n",
    "utils.checkBatches(df,weight_column='event_weight',batch_size=parameters['batch_size'])\n",
    "print ('Using training weight')\n",
    "utils.checkBatches(df,weight_column='training_weight',batch_size=parameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the background and signal weights #\n",
    "fig,axs = plt.subplots(figsize=(16,8),nrows=2,ncols=3)\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.98, bottom=0.1, wspace=0.2,hspace=0.3)\n",
    "for irow,label in enumerate([0,1]):\n",
    "    for icol,column in enumerate(['weight','event_weight','training_weight']):\n",
    "        axs[irow,icol].hist(df[df['label']==label][column],bins=100,color='b')\n",
    "        axs[irow,icol].set_title(f\"Label = {label}\")\n",
    "        axs[irow,icol].set_xlabel(column)\n",
    "        axs[irow,icol].set_yscale('log')\n",
    "fig.savefig(\"event_weights_A.pdf\", dpi = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc5234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the weights per process #\n",
    "if False:\n",
    "    with PdfPages(\"event_weights_B.pdf\") as pdf:\n",
    "        for process in pd.unique(df['process']):\n",
    "            fig,axs = plt.subplots(figsize=(16,6),nrows=1,ncols=3)\n",
    "            fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.2,hspace=0.3)\n",
    "            fig.suptitle(f\"Process {process}\")\n",
    "            for icol,column in enumerate(['weight','event_weight','training_weight']):\n",
    "                axs[icol].hist(df[df['process']==process][column],bins=100,color='b')\n",
    "                axs[icol].set_xlabel(column)\n",
    "                axs[icol].set_xlim(0,(df[df['process']==process][column]).max()*1.5)\n",
    "                axs[icol].set_yscale('log')\n",
    "            pdf.savefig()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[input_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ddd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine splitting variable #\n",
    "split_var = df['Phi_ph1'].copy()\n",
    "split_var *= 1e4\n",
    "split_var -= np.floor(split_var) \n",
    "split_var = (split_var*1e1).astype(int)\n",
    "split_var = split_var %2 == 0\n",
    "print (f'Even set has {df[split_var].shape[0]:10d} events [{df[split_var].shape[0]/df.shape[0]*100:5.2f}%]')\n",
    "print (f'Odd  set has {df[~split_var].shape[0]:10d} events [{df[~split_var].shape[0]/df.shape[0]*100:5.2f}%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd045db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets splitting #\n",
    "print (f'Using split type {split}')\n",
    "# split_var = True (even number) | False (odd number)\n",
    "# Name of the model is related to the even | odd quality of the events during inference (ie, in bamboo)\n",
    "if split == 'even':\n",
    "    train_df = df[~split_var] # Trained on odd\n",
    "    test_df  = df[split_var]  # Evaluated on even \n",
    "elif split == 'odd':\n",
    "    train_df = df[split_var]  # Trained on even\n",
    "    test_df  = df[~split_var] # Evaluated on odd \n",
    "else:\n",
    "    raise RuntimeError(f'Split needs to be either odd or even, is {split}')\n",
    "\n",
    "# Randomize for training (always good to randomize) #\n",
    "train_df = train_df.sample(frac=1)\n",
    "\n",
    "# Quantile corrections #\n",
    "# When an event has a large weight, it can imbalance a lot the training, still the weight might have a meaning\n",
    "# Idea : instead of 1 event with wi>>1, we use N copies of the event with wf = wi/N\n",
    "# From the point of view of the physics it does not matter, the total event weight sum of each process is the same\n",
    "# From the point of view of the DNN, we have split a tough nut to crack into several smaller ones\n",
    "\n",
    "quantile_lim = train_df['training_weight'].quantile(quantile)\n",
    "print (f'{(1-quantile)*100:5.2f}% right quantile is when weight is at {quantile_lim}')\n",
    "print ('  -> These events will be repeated and their learning weights reduced accordingly to avoid unstability') \n",
    "\n",
    "# Select the events #\n",
    "idx_to_repeat = train_df['training_weight'] >= quantile_lim                          \n",
    "events_excess = train_df[idx_to_repeat].copy()\n",
    "\n",
    "saved_columns = train_df[['training_weight','process']].copy()\n",
    "\n",
    "# Compute multiplicative factor #\n",
    "factor = (events_excess['training_weight']/quantile_lim).values.astype(np.int32) \n",
    "\n",
    "# Correct the weights of events already in df #\n",
    "train_df.loc[idx_to_repeat,'training_weight'] /= factor\n",
    "\n",
    "# Add N-1 copies #\n",
    "arr_to_repeat = train_df[idx_to_repeat].values                                       \n",
    "repetition = np.repeat(np.arange(arr_to_repeat.shape[0]), factor-1)                   \n",
    "df_repeated = pd.DataFrame(np.take(arr_to_repeat,repetition,axis=0),columns=train_df.columns)\n",
    "df_repeated = df_repeated.astype(train_df.dtypes.to_dict()) # otherwise dtypes are object\n",
    "train_df = pd.concat((train_df,df_repeated),axis=0,ignore_index=True).sample(frac=1).reset_index() # Add and randomize\n",
    "\n",
    "# Printout #\n",
    "print ('Changes per process in training set')\n",
    "for process in pd.unique(train_df['process']):\n",
    "    N_before = saved_columns[saved_columns['process']==process].shape[0]\n",
    "    N_after  = train_df[train_df['process']==process].shape[0]\n",
    "    if N_before != N_after:\n",
    "        print (f\"{process:20s}\")\n",
    "        print (f\"... {N_before:6d} events [sum weight = {saved_columns[saved_columns['process']==process]['training_weight'].sum():14.6f}]\",end=' -> ')\n",
    "        print (f\"{N_after:6d} events [sum weight = {train_df[train_df['process']==process]['training_weight'].sum():14.6f}]\")\n",
    "    \n",
    "print ()\n",
    "print (f\"Total entries : {saved_columns.shape[0]:14d} -> {train_df.shape[0]:14d}\")\n",
    "print (f\"Total event sum : {saved_columns['training_weight'].sum():14.6f} -> {train_df['training_weight'].sum():14.6f}\")\n",
    "\n",
    "# Validation split #\n",
    "train_df,val_df  = train_test_split(train_df,test_size=0.3)\n",
    "\n",
    "# Printout #\n",
    "print ('\\nFinal sets')\n",
    "print (f'Training set   = {train_df.shape[0]}')\n",
    "print (f'Validation set = {val_df.shape[0]}')\n",
    "print (f'Testing set    = {test_df.shape[0]}')\n",
    "print (f'Total set      = {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b20e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the background and signal weights #\n",
    "fig,axs = plt.subplots(figsize=(16,8),nrows=1,ncols=2)\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.98, bottom=0.1, wspace=0.2,hspace=0.3)\n",
    "\n",
    "if split == 'even':\n",
    "    axs[0].hist(df[~split_var]['training_weight'],bins=100,color='b')\n",
    "elif split == 'odd':\n",
    "    axs[0].hist(df[split_var]['training_weight'],bins=100,color='b')\n",
    "axs[0].set_title(\"Before correction\")\n",
    "axs[0].set_xlabel(\"Training weight\")\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].hist(train_df['training_weight'],bins=100,color='b')\n",
    "axs[1].set_title(\"After correction\")\n",
    "axs[1].set_xlabel(\"Training weight\")\n",
    "axs[1].set_yscale('log')\n",
    "fig.savefig(\"event_weights_C.pdf\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer #\n",
    "inputs = keras.Input(shape=(len(input_vars),), name=\"particles\")\n",
    "\n",
    "# Preprocessing layer\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "normalizer = preprocessing.Normalization(mean     = train_df[input_vars].mean(axis=0),\n",
    "                                         variance = train_df[input_vars].var(axis=0),\n",
    "                                         name     = 'Normalization')(inputs)\n",
    "    # this layer does the preprocessing (x-mu)/std for each input\n",
    "# Dense (hidden) layers #\n",
    "x = normalizer\n",
    "for i in range(parameters['n_layers']):\n",
    "    x = layers.Dense(units                = parameters['n_neurons'], \n",
    "                     activation           = parameters['hidden_activation'], \n",
    "                     activity_regularizer = tf.keras.regularizers.l2(parameters['l2']),\n",
    "                     name                 = f\"dense_{i}\")(x)\n",
    "    if parameters['batch_norm']:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    if parameters['dropout'] > 0.:\n",
    "        x = layers.Dropout(parameters['dropout'])(x)\n",
    "# Output layer #\n",
    "outputs = layers.Dense(units                = 1, \n",
    "                       activation           = parameters['output_activation'],\n",
    "                       activity_regularizer = tf.keras.regularizers.l2(parameters['l2']),\n",
    "                       name                 = \"predictions\")(x)\n",
    "\n",
    "# Registering the model #\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preprocess = keras.Model(inputs=inputs, outputs=normalizer)\n",
    "out_test = model_preprocess.predict(train_df[input_vars],batch_size=5000)\n",
    "print ('Input (after normalization) mean (should be close to 0)')\n",
    "print (out_test.mean(axis=0))\n",
    "print ('Input (after normalization) variance (should be close to 1)')\n",
    "print (out_test.var(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40426231",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    #optimizer=keras.optimizers.RMSprop(),\n",
    "    optimizer=keras.optimizers.Adam(lr=parameters['lr']),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.BinaryAccuracy(),\n",
    "             tf.keras.metrics.AUC(),\n",
    "             tf.keras.metrics.Precision(),\n",
    "             tf.keras.metrics.Recall()],\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks #\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                               min_delta = 0.001, \n",
    "                               patience = 20,\n",
    "                               verbose=1,\n",
    "                               mode='min',\n",
    "                               restore_best_weights=True)\n",
    "# Stop the learning when val_loss stops increasing \n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "reduce_plateau = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                   factor = 0.1,\n",
    "                                   min_delta = 0.001, \n",
    "                                   patience = 8,\n",
    "                                   min_lr = 1e-8,\n",
    "                                   verbose=2,\n",
    "                                   mode='min')\n",
    "# reduce LR if not improvement for some time \n",
    "# https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "import History \n",
    "importlib.reload(History)\n",
    "loss_history = History.LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa63501",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_df[input_vars],\n",
    "    train_df['label'],\n",
    "    verbose=2,\n",
    "    batch_size=parameters['batch_size'],\n",
    "    epochs=parameters['epochs'],\n",
    "    sample_weight=train_df['training_weight'],\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(val_df[input_vars],val_df['label'],val_df['training_weight']),\n",
    "    callbacks = [early_stopping, reduce_plateau, loss_history],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94334845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "History.PlotHistory(loss_history,params=parameters,outputName=f'loss_{suffix}_{split}.png')\n",
    "# Params is a dict of parameters with name and values\n",
    "# used for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb255ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce output on the test set as new column #\n",
    "output = model.predict(test_df[input_vars],batch_size=5000)\n",
    "    # Here the batch_size arg is independent of the learning\n",
    "    # Default is 32, but it can become slow, by using large value it will just compute more values in parallel\n",
    "    # (more or less parallel, we are not using a GPU)\n",
    "if 'output' in test_df.columns:\n",
    "    # If already output, need to remove to add again\n",
    "    # avoid issues in case you run this cell multiple times\n",
    "    del test_df['output']\n",
    "test_df = pd.concat((test_df,pd.DataFrame(output,columns=['output'],index=test_df.index)),axis=1)\n",
    "# We add the output as a column, a bit messy, different ways, here I use a concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import roc\n",
    "importlib.reload(roc) # Reload in case file has changed\n",
    "roc.rocAndSig(y_true     = test_df['label'],\n",
    "              y_pred     = test_df['output'],\n",
    "              w_roc      = test_df['training_weight'],\n",
    "              w_sig      = test_df['event_weight'],\n",
    "              outputName = f'roc_{suffix}_{split}.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(figsize=(16,8),nrows=1,ncols=2)\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.98, bottom=0.1, wspace=0.2,hspace=0.3)\n",
    "\n",
    "sig_df = test_df[test_df['label']==1]\n",
    "bkg_df = test_df[test_df['label']==0]\n",
    "\n",
    "# Manual binning so we can compute significance #\n",
    "bins = np.linspace(0,1,51)\n",
    "centers = (bins[1:]+bins[:-1])/2\n",
    "widths = np.diff(bins)\n",
    "\n",
    "\n",
    "def get_bin_content(y,w):\n",
    "    digitized = np.digitize(y,bins)\n",
    "    return np.array([w[digitized==i].sum() for i in range(1, len(bins))])\n",
    "\n",
    "for icol,weight in enumerate(['event_weight','training_weight']):\n",
    "    # Fill the bins myself #\n",
    "    b = get_bin_content(bkg_df['output'],bkg_df[weight])\n",
    "    s = get_bin_content(sig_df['output'],sig_df[weight])\n",
    "    cumsum_s_left = np.cumsum(s)/s.sum()\n",
    "    cumsum_b_left = np.cumsum(b)/b.sum()\n",
    "    cumsum_s_right = np.cumsum(s[::-1])[::-1]/s.sum()\n",
    "    cumsum_b_right = np.cumsum(b[::-1])[::-1]/b.sum()\n",
    "    # Need to integrate all the bins right of the DNN cut to get significance\n",
    "    z_left = np.nan_to_num(np.sqrt(2*((cumsum_s_left+cumsum_b_left)*np.log(1+cumsum_s_left/cumsum_b_left)-cumsum_s_left)))\n",
    "    z_right = np.nan_to_num(np.sqrt(2*((cumsum_s_right+cumsum_b_right)*np.log(1+cumsum_s_right/cumsum_b_right)-cumsum_s_right)))\n",
    "    z_left /= z_left.max()\n",
    "    z_right /= z_right.max()\n",
    "    axs[icol].bar(x=centers,height=b,width=widths,alpha=0.5,color='b',label='Bkg')\n",
    "    axs[icol].bar(x=centers,height=s,width=widths,alpha=0.5,color='g',label=r'$HH \\rightarrow WW \\gamma \\gamma$')\n",
    "    ax2=axs[icol].twinx()   \n",
    "    ax2.plot(centers,z_left,color='r',label='Significance (left of cut) [normed]')\n",
    "    ax2.plot(centers,cumsum_s_left,color='g',label='Signal content (left of cut)')\n",
    "    ax2.plot(centers,cumsum_b_left,color='b',label='Bkg content (left of cut)')\n",
    "    ax2.plot(centers,z_right,color='r',linestyle='--',label='Significance (right of cut) [normed]')\n",
    "    ax2.plot(centers,cumsum_s_right,color='g',linestyle='--',label='Signal content (right of cut)')\n",
    "    ax2.plot(centers,cumsum_b_right,color='b',linestyle='--',label='Bkg content (right of cut)')\n",
    "    #ax2.set_yscale(\"log\")\n",
    "    #ax2.set_ylim([0,z.max()*1.1])\n",
    "    ax2.set_ylim([0,1.4])\n",
    "    ax2.set_ylabel('CDF')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    axs[icol].set_title(f\"Using {weight}\")\n",
    "    axs[icol].set_xlabel('DNN output')\n",
    "    axs[icol].set_ylabel('Yield')\n",
    "    axs[icol].set_ylim([s.min()*0.1,np.maximum(s,b).max()*100])\n",
    "    axs[icol].set_yscale('log')\n",
    "    axs[icol].legend(loc='upper left')\n",
    "fig.savefig(f\"prediction_{suffix}_{split}.pdf\", dpi = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(test_df[input_vars], \n",
    "                        test_df['label'], \n",
    "                        sample_weight = test_df['training_weight'], \n",
    "                        batch_size = 5000,\n",
    "                        verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "modelName = f\"model_{suffix}_{split}\"\n",
    "model.save(modelName)\n",
    "print(f\"Saved model to disk as {modelName}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
